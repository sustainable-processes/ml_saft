{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training a neural network on QM9\n",
    "\n",
    "This tutorial will explain how to use SchNetPack for training a model\n",
    "on the QM9 dataset and how the trained model can be used for further applications.\n",
    "\n",
    "First, we import the necessary modules and create a new directory for the data and our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jr629406/anaconda3/envs/dl4thermo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import schnetpack as spk\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "As explained in the [previous tutorial](tutorial_01_preparing_data.ipynb), datasets in SchNetPack are loaded with the `AtomsLoader` class or one of the sub-classes that are specialized for common benchmark datasets. \n",
    "The `QM9` dataset class will download and convert the data. We will only use the inner energy at 0K `U0`, so all other properties do not need to be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "qm9tut = './qm9tut'\n",
    "if not os.path.exists('qm9tut'):\n",
    "    os.makedirs(qm9tut)\n",
    "\n",
    "%rm qm9tut/split.npz\n",
    "\n",
    "qm9data = QM9(\n",
    "    f'{qm9tut}/qm9.db', \n",
    "    batch_size=100,\n",
    "    num_train=110000,\n",
    "    num_val=10000,\n",
    "    transforms=[\n",
    "        trn.SubtractCenterOfMass(),\n",
    "        #trn.ASENeighborList(cutoff=5.),\n",
    "        trn.MatScipyNeighborList(cutoff=5.),\n",
    "        #trn.RemoveOffsets(QM9.mu, remove_mean=True, remove_atomrefs=True),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    property_units={QM9.mu: 'Debye'},\n",
    "    num_workers=0,\n",
    "    split_file=os.path.join(qm9tut, \"split.npz\"),\n",
    "    pin_memory=False, # set to false, when not using a GPU\n",
    "    load_properties=[QM9.mu], #only load mu property\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset is downloaded and partitioned automatically. PyTorch `DataLoader`s can be obtained using `qm9data.train_dataloader()`, `qm9data.val_dataloader()` and `qm9data.test_dataloader()`.\n",
    "\n",
    "Before building the model, we remove offsets from the energy for good initial conditions. We will get this from the training dataset. Above, this is done automatically by the `RemoveOffsets` transform.\n",
    "In the following we show what happens under the hood.\n",
    "For QM9, we also have single-atom reference values stored in the metadata:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "atomrefs = qm9data.train_dataset.atomrefs\n",
    "print('Mu of hyrogen:', atomrefs[QM9.mu][1].item(), 'Debye')\n",
    "print('Mu of carbon:', atomrefs[QM9.mu][6].item(), 'Debye')\n",
    "print('Mu of oxygen:', atomrefs[QM9.mu][8].item(), 'Debye')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These can be used together with the mean and standard deviation of the energy per atom to initialize the model with a good guess of the energy of a molecule. When calculating these statistics, we pass the atomref to take into account, that the model will add these atomrefs to the predicted energy later, so that this part of the energy does not have to be considered in the statistics, i.e.\n",
    "\\begin{equation}\n",
    "\\mu_{U_0} = \\frac{1}{n_\\text{train}} \\sum_{n=1}^{n_\\text{train}} \\left( U_{0,n} - \\sum_{i=1}^{n_{\\text{atoms},n}} U_{0,Z_{n,i}} \\right)\n",
    "\\end{equation}\n",
    "for the mean and analogously for the standard deviation. In this case, this corresponds to the mean and std. dev of the *atomization energy* per atom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "means, stddevs = qm9data.get_stats(\n",
    "    QM9.mu, divide_by_atoms=True, remove_atomref=True\n",
    ")\n",
    "print('Mean dipole moment / atom:', means.item())\n",
    "print('Std. dev. dipole moment / atom:', stddevs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the model\n",
    "\n",
    "Next, we need to build the model and define how it should be trained.\n",
    "\n",
    "In SchNetPack, a neural network potential usually consists of three parts:\n",
    "\n",
    "1. A list of input modules that prepare the batched data before the building the representation.\n",
    "   This includes, e.g., the calculation of pairwise distances between atoms based on neighbor indices or add auxiliary\n",
    "   inputs for response properties.\n",
    "2. The representation which either constructs atom-wise features, e.g. with SchNet or PaiNN.\n",
    "3. One or more output modules for property prediction.\n",
    "\n",
    "Here, we use the `SchNet` representation with 3 interaction layers, a 5 Angstrom cosine cutoff with pairwise distances\n",
    "expanded on 20 Gaussians and 50 atomwise features and convolution filters, since we only have a few\n",
    "training examples. Then, we use an `Atomwise` module to predict the inner energy $U_0$ by summing over atom-wise\n",
    "energy contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cutoff = 5.\n",
    "n_atom_basis = 128\n",
    "\n",
    "pairwise_distance = spk.atomistic.PairwiseDistances() # calculates pairwise distances between atoms\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "painn = spk.representation.PaiNN(\n",
    "    n_atom_basis=n_atom_basis, n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff)\n",
    ")\n",
    "#pred_U0 = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=QM9.U0)\n",
    "pred_mu = spk.atomistic.DipoleMoment(n_in=n_atom_basis, dipole_key='dipole_moment', predict_magnitude=True, use_vector_representation=False)\n",
    "\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=painn,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_mu],\n",
    "    postprocessors=[trn.CastTo64()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The last argument here is a list of postprocessors that will only be used if `nnpot.inference_mode=True` is set.\n",
    "It will not be used in training or validation, but only for predictions.\n",
    "Here, this is used to deal with numerical accuracy and normalization of model outputs:\n",
    "To make training easier, we have subtracted single atom energies as well as the mean energy per atom\n",
    "in the preprocessing (see above).\n",
    "This does not matter for the loss, but for the final prediction we want to get the real energies.\n",
    "Additionally, we have removed the energy offsets *before* casting to float32 in the preprocessor.\n",
    "This avoids loss of numerical precision.\n",
    "Analog to this, we also have to first cast to float64, before re-adding the offsets in the post-processor\n",
    "\n",
    "The output modules store the prediction in a dictionary under the `output_key` (here: `QM9.U0`), which is connected to\n",
    "a target property with loss functions and evaluation metrics using the `ModelOutput` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_mu = spk.task.ModelOutput(\n",
    "    name=QM9.mu,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1.,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By default, the target is assumed to have the same name as the output. Otherwise, a different `target_name`\n",
    "has to be provided.\n",
    "Here, we already gave the output the same name as the target in the dataset (`QM9.U0`).\n",
    "In case of multiple outputs, the full loss is a weighted sum of all output losses.\n",
    "Therefore, it is possible to provide a `loss_weight`, which we here just set to 1.\n",
    "\n",
    "All components defined above are then passed to `AtomisticTask`, which is a sublass of\n",
    "[`LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "This connects the model and training process and can then be passed to the PyTorch Lightning `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jr629406/anaconda3/envs/dl4thermo/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_mu],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Now, the model is ready for training. Since we already defined all necessary components, the only thing left to do is\n",
    "passing it to the PyTorch Lightning `Trainer` together with the data module.\n",
    "\n",
    "Additionally, we can provide callbacks that take care of logging, checkpointing etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir=qm9tut)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=os.path.join(qm9tut, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=qm9tut,\n",
    "    max_epochs=100000, # for testing, we restrict the number of epochs\n",
    "    gpus=1,\n",
    ")\n",
    "trainer.fit(task, datamodule=qm9data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `ModelCheckpoint` of SchNetPack is equivalent to that in PyTorch Lightning,\n",
    "except that we also store the best inference model. We will show how to use this in the next section.\n",
    "\n",
    "You can have a look at the training log using Tensorboard:\n",
    "```\n",
    "tensorboard --logdir=qm9tut/default\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing\n",
    "\n",
    "Having trained a model for QM9, we are going to use it to obtain some predictions.\n",
    "First, we need to load the model. The `Trainer` stores the best model in the model directory which can be loaded using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "\n",
    "best_model = torch.load(os.path.join(qm9tut, 'best_inference_model'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use the test dataloader from the QM( data to obtain a batch of molecules and apply the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def infer(dataloader):\n",
    "    reals = torch.tensor([])\n",
    "    preds = torch.tensor([])\n",
    "    for batch in dataloader:\n",
    "        real = batch[QM9.mu]\n",
    "        pred = best_model(batch)[\"dipole_moment\"]\n",
    "        reals = torch.cat([reals, real])\n",
    "        preds = torch.cat([preds, pred])\n",
    "    return reals, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reals, test_preds = infer(qm9data.test_dataloader())\n",
    "print(f\"Test MAE: {torch.mean(torch.abs(test_reals-test_preds))}\")\n",
    "print(f\"Test MSE: {torch.mean(torch.pow(test_reals-test_preds,2))}\")\n",
    "val_reals, val_preds = infer(qm9data.val_dataloader())\n",
    "print(f\"Val MAE: {torch.mean(torch.abs(val_reals-val_preds))}\")\n",
    "print(f\"Val MSE: {torch.mean(torch.pow(val_reals-val_preds,2))}\")\n",
    "train_reals, train_preds = infer(qm9data.train_dataloader())\n",
    "print(f\"Train MAE: {torch.mean(torch.abs(train_reals-train_preds))}\")\n",
    "print(f\"Train MSE: {torch.mean(torch.pow(train_reals-train_preds,2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4thermo_kernel",
   "language": "python",
   "name": "dl4thermo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbsphinx": {
   "execute": "never"
  },
  "vscode": {
   "interpreter": {
    "hash": "83510ad8c38e48978a75779889aee48d882d4f29bd21dcf159e9ea69604d26db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
